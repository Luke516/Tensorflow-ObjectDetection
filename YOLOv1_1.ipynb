{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_name =  [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \n",
    "                 \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \n",
    "                 \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \n",
    "                 \"sheep\", \"sofa\", \"train\",\"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#if you have multiple GPU on the machine, choose only one to use on this notebook\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "#let the gpu allocates memory space dynamically\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction=0.5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DatasetRunner:\n",
    "    \"\"\"\n",
    "    Load pascalVOC 2007 dataset and creates an input pipeline ready to be fed into a model.\n",
    "    - Reshapes images into 448 x 448\n",
    "    - converts [0 1] to [-1 1]\n",
    "    - shuffles the input\n",
    "    - builds batches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, common_params, dataset_params):\n",
    "        \n",
    "        self.width = common_params['image_size']\n",
    "        self.height = common_params['image_size']\n",
    "        self.batch_size = common_params['batch_size']\n",
    "        self.num_classes = common_params['num_classes']\n",
    "        self.data_path = dataset_params['path']\n",
    "        self.thread_num = dataset_params['thread_num']\n",
    "        self.image_dir = dataset_params['image_dir']\n",
    "        \n",
    "        self.max_objects = common_params['max_objects_per_image']\n",
    "            \n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph, config=config)\n",
    "        \n",
    "        self.image_names = []\n",
    "        self.record_list = []\n",
    "        self.object_num_list = []\n",
    "        # filling the record_list\n",
    "        input_file = open(self.data_path, 'r')\n",
    "        \n",
    "        for line in input_file:\n",
    "            line = line.strip()\n",
    "            ss = line.split(' ')\n",
    "            self.image_names.append(ss[0])\n",
    "\n",
    "            self.record_list.append([float(num) for num in ss[1:]])\n",
    "\n",
    "            self.object_num_list.append(min(len(self.record_list[-1])//5, self.max_objects))\n",
    "            if len(self.record_list[-1])<self.max_objects*5:\n",
    "                self.record_list[-1] = self.record_list[-1] +\\\n",
    "                [float(0), float(0), float(0), float(0), float(0)]*\\\n",
    "                (self.max_objects-len(self.record_list[-1])//5)\n",
    "                \n",
    "            elif len(self.record_list[-1])>self.max_objects*5:\n",
    "                self.record_list[-1] = self.record_list[-1][:self.max_objects*5]\n",
    "\n",
    "        self.build_train_data_tensor()\n",
    "            \n",
    "    def build_train_data_tensor(self):\n",
    "        \n",
    "        def data_generator(image_name, raw_labels, object_num):\n",
    "            image_file = tf.read_file(self.image_dir+image_name)\n",
    "            image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "\n",
    "            h = tf.shape(image)[0]\n",
    "            w = tf.shape(image)[1]\n",
    "\n",
    "            width_rate = self.width * 1.0 / tf.cast(w, tf.float32) \n",
    "            height_rate = self.height * 1.0 / tf.cast(h, tf.float32) \n",
    "\n",
    "            image = tf.image.resize_images(image, size=[self.height,self.width])\n",
    "            \n",
    "            raw_labels = tf.cast(tf.reshape(raw_labels, [-1, 5]), tf.float32)\n",
    "            \n",
    "            xmin = raw_labels[:, 0]\n",
    "            ymin = raw_labels[:, 1]\n",
    "            xmax = raw_labels[:, 2]\n",
    "            ymax = raw_labels[:, 3]\n",
    "            class_num = raw_labels[:, 4]\n",
    "            \n",
    "            xcenter = (xmin + xmax) * 1.0 / 2.0 * width_rate\n",
    "            ycenter = (ymin + ymax) * 1.0 / 2.0 * height_rate\n",
    "            \n",
    "            box_w = (xmax - xmin) * width_rate\n",
    "            box_h = (ymax - ymin) * height_rate\n",
    "            \n",
    "            labels = tf.stack([xcenter, ycenter, box_w, box_h, class_num], axis = 1)\n",
    "\n",
    "            return image, labels, tf.cast(object_num, tf.int32)\n",
    "            \n",
    "\n",
    "        with self.graph.as_default():\n",
    "            \n",
    "            dataset = tf.data.Dataset.from_tensor_slices((self.image_names, \n",
    "                                                          np.array(self.record_list), \n",
    "                                                          np.array(self.object_num_list)))\n",
    "\n",
    "            dataset = dataset.map(data_generator, num_parallel_calls = self.thread_num)\n",
    "            dataset = dataset.batch(self.batch_size)\n",
    "            dataset = dataset.shuffle(self.batch_size) \n",
    "            dataset = dataset.repeat()\n",
    "\n",
    "            self.iterator = tf.data.Iterator.from_structure(dataset.output_types)  \n",
    "            self.data_init_op = self.iterator.make_initializer(dataset)\n",
    "\n",
    "            self.sess.run(self.data_init_op)\n",
    "            self.iterate_op = self.iterator.get_next()\n",
    "            \n",
    "    def batch(self):\n",
    "        images, labels, objects_num = self.sess.run(self.iterate_op)\n",
    "\n",
    "        while objects_num.shape[0] < self.batch_size:\n",
    "            images, labels, objects_num = self.sess.run(self.iterate_op)\n",
    "        \n",
    "        images = images/255 * 2 - 1\n",
    "        \n",
    "        return images, labels, objects_num\n",
    "        \n",
    "        \n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def close(self):\n",
    "\n",
    "        self.sess.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YoloTinyNet(object):\n",
    "\n",
    "    def __init__(self, common_params, net_params, test=False):\n",
    "        \"\"\"\n",
    "        common params: a params dict\n",
    "        net_params   : a params dict\n",
    "        \"\"\"\n",
    "        #pretrained variable collection\n",
    "        self.pretrained_collection = []\n",
    "        #trainable variable collection\n",
    "        self.trainable_collection = []\n",
    "         \n",
    "        #process params\n",
    "        self.image_size = int(common_params['image_size'])\n",
    "        self.num_classes = int(common_params['num_classes'])\n",
    "        self.cell_size = int(net_params['cell_size'])\n",
    "        self.boxes_per_cell = int(net_params['boxes_per_cell'])\n",
    "        self.batch_size = int(common_params['batch_size'])\n",
    "        self.weight_decay = float(net_params['weight_decay'])\n",
    "\n",
    "        if not test:\n",
    "            self.object_scale = float(net_params['object_scale'])\n",
    "            self.noobject_scale = float(net_params['noobject_scale'])\n",
    "            self.class_scale = float(net_params['class_scale'])\n",
    "            self.coord_scale = float(net_params['coord_scale'])\n",
    "         \n",
    "    def _variable_on_cpu(self, name, shape, initializer, pretrain=True, train=True):\n",
    "        \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "        Args:\n",
    "          name: name of the Variable\n",
    "          shape: list of ints\n",
    "          initializer: initializer of Variable\n",
    "        Returns:\n",
    "          Variable Tensor\n",
    "        \"\"\"\n",
    "        with tf.device('/cpu:0'):\n",
    "            var = tf.get_variable(name, shape, initializer=initializer, dtype=tf.float32)\n",
    "            if pretrain:\n",
    "                self.pretrained_collection.append(var)\n",
    "            if train:\n",
    "                self.trainable_collection.append(var)\n",
    "        return var \n",
    "    \n",
    "    def _variable_with_weight_decay(self, name, shape, stddev, wd, pretrain=True, train=True):\n",
    "        \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "        Note that the Variable is initialized with truncated normal distribution\n",
    "        A weight decay is added only if one is specified.\n",
    "        Args:\n",
    "          name: name of the variable \n",
    "          shape: list of ints\n",
    "          stddev: standard devision of a truncated Gaussian\n",
    "          wd: add L2Loss weight decay multiplied by this float. If None, weight \n",
    "          decay is not added for this Variable.\n",
    "       Returns:\n",
    "          Variable Tensor \n",
    "        \"\"\"\n",
    "        var = self._variable_on_cpu(name, shape,\n",
    "            tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32), pretrain, train)\n",
    "#         if wd is not None:\n",
    "#             weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "#             tf.add_to_collection('losses', weight_decay)\n",
    "        return var \n",
    "\n",
    "    def fully(self, scope, input, in_dimension, out_dimension, leaky=True, pretrain=True, train=True):\n",
    "        \"\"\"Fully connection layer\n",
    "        Args:\n",
    "          scope: variable_scope name\n",
    "          input: [batch_size, ???]\n",
    "          out_dimension: int32\n",
    "        Return:\n",
    "          output: 2-D tensor [batch_size, out_dimension]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(scope) as scope:\n",
    "            reshape = tf.reshape(input, [tf.shape(input)[0], -1])\n",
    "\n",
    "            weights = self._variable_with_weight_decay('weights', shape=[in_dimension, out_dimension],\n",
    "                                  stddev=0.04, wd=self.weight_decay, pretrain=pretrain, train=train)\n",
    "            biases = self._variable_on_cpu('biases', [out_dimension], tf.constant_initializer(0.0), pretrain, train)\n",
    "            local = tf.matmul(reshape, weights) + biases\n",
    "\n",
    "            if leaky:\n",
    "                local = self.leaky_relu(local)\n",
    "            else:\n",
    "                local = tf.identity(local, name=scope.name)\n",
    "\n",
    "        return local\n",
    "    \n",
    "    def leaky_relu(self, x, alpha=0.1, dtype=tf.float32):\n",
    "        \"\"\"leaky relu \n",
    "        if x > 0:\n",
    "          return x\n",
    "        else:\n",
    "          return alpha * x\n",
    "        Args:\n",
    "          x : Tensor\n",
    "          alpha: float, the slope of the leaky function\n",
    "        Return:\n",
    "          y : Tensor\n",
    "        \"\"\"\n",
    "        x = tf.cast(x, dtype=dtype)\n",
    "        \n",
    "        return tf.nn.leaky_relu(x, alpha=alpha)\n",
    "    \n",
    "    def inference(self, images):\n",
    "        \"\"\"Build the yolo model\n",
    "        Input the images, output prediction boxes(center_x, center_y, w, h, scale) and the corresponding classes\n",
    "        \n",
    "        Args:\n",
    "          images:  4-D tensor [batch_size, image_height, image_width, channels]\n",
    "        Returns:\n",
    "          predicts: 4-D tensor [batch_size, cell_size, cell_size, num_classes + 5 * boxes_per_cell]\n",
    "        \"\"\"\n",
    "        images = tf.reshape(images, [-1, self.image_size*self.image_size*3])\n",
    "        \n",
    "        #Only Fully connected layer\n",
    "        fully0 = self.fully('local0', images, self.image_size*self.image_size*3, 256)\n",
    "\n",
    "        fully1 = self.fully('local1', fully0, 256, 512)\n",
    "\n",
    "        fully2 = self.fully('local2', fully1, 512, 4096)\n",
    "\n",
    "        fully3 = self.fully('local3', fully2, 4096, self.cell_size * self.cell_size * \n",
    "                            (self.num_classes + self.boxes_per_cell * 5), leaky=False,\n",
    "                            pretrain=False, train=True)\n",
    "\n",
    "        n1 = self.cell_size * self.cell_size * self.num_classes\n",
    "\n",
    "        n2 = n1 + self.cell_size * self.cell_size * self.boxes_per_cell\n",
    "\n",
    "        class_probs = tf.reshape(fully3[:, 0:n1], (-1, self.cell_size, self.cell_size, self.num_classes))\n",
    "        scales = tf.reshape(fully3[:, n1:n2], (-1, self.cell_size, self.cell_size, self.boxes_per_cell))\n",
    "        boxes = tf.reshape(fully3[:, n2:], (-1, self.cell_size, self.cell_size, self.boxes_per_cell * 4))\n",
    "\n",
    "        predicts = tf.concat([class_probs, scales, boxes], 3)\n",
    "\n",
    "        return predicts\n",
    "\n",
    "    def iou(self, boxes1, boxes2):\n",
    "        \"\"\"calculate ious\n",
    "        Args:\n",
    "          boxes1: 4-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "          boxes2: 1-D tensor [4] ===> (x_center, y_center, w, h)\n",
    "          \n",
    "        Return:\n",
    "          iou: 3-D tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        \"\"\"\n",
    "        \n",
    "        #boxes1 : [4(xmin, ymin, xmax, ymax), cell_size, cell_size, boxes_per_cell]\n",
    "        boxes1 = tf.stack([boxes1[:, :, :, 0] - boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] - boxes1[:, :, :, 3] / 2,\n",
    "                          boxes1[:, :, :, 0] + boxes1[:, :, :, 2] / 2, boxes1[:, :, :, 1] + boxes1[:, :, :, 3] / 2])\n",
    "        \n",
    "        #boxes1 : [cell_size, cell_size, boxes_per_cell, 4(xmin, ymin, xmax, ymax)]\n",
    "        boxes1 = tf.transpose(boxes1, [1, 2, 3, 0])\n",
    "\n",
    "        boxes2 =  tf.stack([boxes2[0] - boxes2[2] / 2, boxes2[1] - boxes2[3] / 2,\n",
    "                          boxes2[0] + boxes2[2] / 2, boxes2[1] + boxes2[3] / 2])\n",
    "\n",
    "        #calculate the left up point of boxes' overlap area\n",
    "        lu = tf.maximum(boxes1[:, :, :, 0:2], boxes2[0:2])\n",
    "        #calculate the right down point of boxes overlap area\n",
    "        rd = tf.minimum(boxes1[:, :, :, 2:], boxes2[2:])\n",
    "\n",
    "        #intersection\n",
    "        intersection = rd - lu \n",
    "\n",
    "        #the size of the intersection area\n",
    "        inter_square = intersection[:, :, :, 0] * intersection[:, :, :, 1]\n",
    "        \n",
    "        mask = tf.cast(intersection[:, :, :, 0] > 0, tf.float32) * tf.cast(intersection[:, :, :, 1] > 0, tf.float32)\n",
    "\n",
    "        #if intersection is negative, then the boxes don't overlap\n",
    "        inter_square = mask * inter_square\n",
    "\n",
    "        #calculate the boxs1 square and boxs2 square\n",
    "        square1 = (boxes1[:, :, :, 2] - boxes1[:, :, :, 0]) * (boxes1[:, :, :, 3] - boxes1[:, :, :, 1])\n",
    "        square2 = (boxes2[2] - boxes2[0]) * (boxes2[3] - boxes2[1])\n",
    "\n",
    "        return inter_square/(square1 + square2 - inter_square + 1e-6)\n",
    "\n",
    "\n",
    "    def losses_calculation(self, num, object_num, loss, predict, labels, nilboy):\n",
    "        \"\"\"\n",
    "        calculate loss\n",
    "        Args:\n",
    "          predict: 3-D tensor [cell_size, cell_size, 5 * boxes_per_cell]\n",
    "          labels : [max_objects, 5]  (x_center, y_center, w, h, class)\n",
    "        \"\"\"\n",
    "        label = labels[num:num+1, :]\n",
    "        label = tf.reshape(label, [-1])\n",
    "\n",
    "        #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "        min_x = (label[0] - label[2] / 2) / (self.image_size / self.cell_size)\n",
    "        max_x = (label[0] + label[2] / 2) / (self.image_size / self.cell_size)\n",
    "\n",
    "        min_y = (label[1] - label[3] / 2) / (self.image_size / self.cell_size)\n",
    "        max_y = (label[1] + label[3] / 2) / (self.image_size / self.cell_size)\n",
    "\n",
    "        min_x = tf.floor(min_x)\n",
    "        min_y = tf.floor(min_y)\n",
    "\n",
    "        max_x = tf.minimum(tf.ceil(max_x), self.cell_size)\n",
    "        max_y = tf.minimum(tf.ceil(max_y), self.cell_size)\n",
    "\n",
    "        temp = tf.cast(tf.stack([max_y - min_y, max_x - min_x]), dtype=tf.int32)\n",
    "        objects = tf.ones(temp, tf.float32)\n",
    "\n",
    "        temp = tf.cast(tf.stack([min_y, self.cell_size - max_y, min_x, self.cell_size - max_x]), tf.int32)\n",
    "        temp = tf.reshape(temp, (2, 2))\n",
    "        objects = tf.pad(objects, temp, \"CONSTANT\")\n",
    "\n",
    "        #calculate objects  tensor [CELL_SIZE, CELL_SIZE]\n",
    "        #calculate responsible tensor [CELL_SIZE, CELL_SIZE]\n",
    "        center_x = label[0] / (self.image_size / self.cell_size)\n",
    "        center_x = tf.floor(center_x)\n",
    "\n",
    "        center_y = label[1] / (self.image_size / self.cell_size)\n",
    "        center_y = tf.floor(center_y)\n",
    "\n",
    "        response = tf.ones([1, 1], tf.float32)\n",
    "\n",
    "        temp = tf.cast(tf.stack([center_y, self.cell_size - center_y - 1, \n",
    "                                 center_x, self.cell_size -center_x - 1]), \n",
    "                       tf.int32)\n",
    "        self.tmp = tf.stack([center_y, self.cell_size - center_y - 1, \n",
    "                             center_x, self.cell_size -center_x - 1])\n",
    "        temp = tf.reshape(temp, (2, 2))\n",
    "        response = tf.pad(response, temp, \"CONSTANT\")\n",
    "        #objects = response\n",
    "\n",
    "        #calculate iou_predict_truth [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        predict_boxes = predict[:, :, self.num_classes + self.boxes_per_cell:]\n",
    "\n",
    "        predict_boxes = tf.reshape(predict_boxes, [self.cell_size, \n",
    "                                                   self.cell_size, \n",
    "                                                   self.boxes_per_cell, 4])\n",
    "\n",
    "        predict_boxes = predict_boxes * [self.image_size / self.cell_size, \n",
    "                                         self.image_size / self.cell_size, \n",
    "                                         self.image_size, self.image_size]\n",
    "\n",
    "        base_boxes = np.zeros([self.cell_size, self.cell_size, 4])\n",
    "\n",
    "        #for each cell\n",
    "        for y in range(self.cell_size):\n",
    "            for x in range(self.cell_size):\n",
    "                \n",
    "                base_boxes[y, x, :] = [self.image_size / self.cell_size * x, self.image_size / self.cell_size * y, 0, 0]\n",
    "                \n",
    "        base_boxes = np.tile(np.resize(base_boxes, [self.cell_size, self.cell_size, 1, 4]), [1, 1, self.boxes_per_cell, 1])\n",
    "\n",
    "        #if there's no predict_box in that cell, then the base_boxes will be calcuated with label and got iou equals 0\n",
    "        predict_boxes = base_boxes + predict_boxes\n",
    "\n",
    "        iou_predict_truth = self.iou(predict_boxes, label[0:4])\n",
    "        #calculate C [cell_size, cell_size, boxes_per_cell]\n",
    "        C = iou_predict_truth * tf.reshape(response, [self.cell_size, self.cell_size, 1])\n",
    "\n",
    "        #calculate I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        I = iou_predict_truth * tf.reshape(response, (self.cell_size, self.cell_size, 1))\n",
    "\n",
    "        max_I = tf.reduce_max(I, 2, keep_dims=True)\n",
    "\n",
    "        I = tf.cast((I >= max_I), tf.float32) * tf.reshape(response, (self.cell_size, self.cell_size, 1))\n",
    "\n",
    "        #calculate no_I tensor [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        no_I = tf.ones_like(I, dtype=tf.float32) - I \n",
    "\n",
    "\n",
    "        p_C = predict[:, :, self.num_classes:self.num_classes + self.boxes_per_cell]\n",
    "\n",
    "        #calculate truth x, y, sqrt_w, sqrt_h 0-D\n",
    "        x = label[0]\n",
    "        y = label[1]\n",
    "\n",
    "        sqrt_w = tf.sqrt(tf.abs(label[2]))\n",
    "        sqrt_h = tf.sqrt(tf.abs(label[3]))\n",
    "\n",
    "        #calculate predict p_x, p_y, p_sqrt_w, p_sqrt_h 3-D [CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "        p_x = predict_boxes[:, :, :, 0]\n",
    "        p_y = predict_boxes[:, :, :, 1]\n",
    "\n",
    "        #p_sqrt_w = tf.sqrt(tf.abs(predict_boxes[:, :, :, 2])) * ((tf.cast(predict_boxes[:, :, :, 2] > 0, tf.float32) * 2) - 1)\n",
    "        #p_sqrt_h = tf.sqrt(tf.abs(predict_boxes[:, :, :, 3])) * ((tf.cast(predict_boxes[:, :, :, 3] > 0, tf.float32) * 2) - 1)\n",
    "        #p_sqrt_w = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 2]))\n",
    "        #p_sqrt_h = tf.sqrt(tf.maximum(0.0, predict_boxes[:, :, :, 3]))\n",
    "        #p_sqrt_w = predict_boxes[:, :, :, 2]\n",
    "        #p_sqrt_h = predict_boxes[:, :, :, 3]\n",
    "        p_sqrt_w = tf.sqrt(tf.minimum(self.image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 2])))\n",
    "        p_sqrt_h = tf.sqrt(tf.minimum(self.image_size * 1.0, tf.maximum(0.0, predict_boxes[:, :, :, 3])))\n",
    "        \n",
    "        #calculate truth p 1-D tensor [NUM_CLASSES]\n",
    "        P = tf.one_hot(tf.cast(label[4], tf.int32), self.num_classes, dtype=tf.float32)\n",
    "\n",
    "        #calculate predict p_P 3-D tensor [CELL_SIZE, CELL_SIZE, NUM_CLASSES]\n",
    "        p_P = predict[:, :, 0:self.num_classes]\n",
    "\n",
    "        #class_loss\n",
    "        class_loss = tf.nn.l2_loss(tf.reshape(objects, (self.cell_size, self.cell_size, 1)) * (p_P - P)) * self.class_scale\n",
    "        #class_loss = tf.nn.l2_loss(tf.reshape(response, (self.cell_size, self.cell_size, 1)) * (p_P - P)) * self.class_scale\n",
    "\n",
    "        #object_loss\n",
    "        object_loss = tf.nn.l2_loss(I * (p_C - C)) * self.object_scale\n",
    "        #object_loss = tf.nn.l2_loss(I * (p_C - (C + 1.0)/2.0)) * self.object_scale\n",
    "\n",
    "        #noobject_loss\n",
    "        #noobject_loss = tf.nn.l2_loss(no_I * (p_C - C)) * self.noobject_scale\n",
    "        noobject_loss = tf.nn.l2_loss(no_I * (p_C)) * self.noobject_scale\n",
    "\n",
    "        #coord_loss\n",
    "        coord_loss = (tf.nn.l2_loss(I * (p_x - x)/(self.image_size/self.cell_size)) +\n",
    "                     tf.nn.l2_loss(I * (p_y - y)/(self.image_size/self.cell_size)) +\n",
    "                     tf.nn.l2_loss(I * (p_sqrt_w - sqrt_w))/ self.image_size +\n",
    "                     tf.nn.l2_loss(I * (p_sqrt_h - sqrt_h))/self.image_size) * self.coord_scale\n",
    "\n",
    "        nilboy = I\n",
    "\n",
    "        return (num + 1, object_num, [loss[0] + class_loss, \n",
    "                                      loss[1] + object_loss, \n",
    "                                      loss[2] + noobject_loss,\n",
    "                                      loss[3] + coord_loss], \n",
    "                predict, labels, nilboy)\n",
    "\n",
    "    def loss(self, predicts, labels, objects_num):\n",
    "        \"\"\"Add Loss to all the trainable variables\n",
    "          Args:\n",
    "          predicts: 4-D tensor [batch_size, cell_size, cell_size, 5 * boxes_per_cell]\n",
    "          ===> (num_classes, boxes_per_cell, 4 * boxes_per_cell)\n",
    "          labels  : 3-D tensor of [batch_size, max_objects, 5]\n",
    "          objects_num: 1-D tensor [batch_size]\n",
    "        \"\"\"\n",
    "        def condition(num, object_num, loss, predict, label, nilboy):\n",
    "            \"\"\"\n",
    "            if num < object_num\n",
    "            \"\"\"\n",
    "            return num < object_num\n",
    "        \n",
    "        class_loss = tf.constant(0, tf.float32)\n",
    "        object_loss = tf.constant(0, tf.float32)\n",
    "        noobject_loss = tf.constant(0, tf.float32)\n",
    "        coord_loss = tf.constant(0, tf.float32)\n",
    "        loss = [0, 0, 0, 0]\n",
    "        for i in range(self.batch_size):\n",
    "            predict = predicts[i, :, :, :]\n",
    "            label = labels[i, :, :]\n",
    "            object_num = objects_num[i]\n",
    "            nilboy = tf.ones([7,7,2])\n",
    "            tuple_results = tf.while_loop(condition, self.losses_calculation, \n",
    "                                          [tf.constant(0), object_num, \n",
    "                                           [class_loss, object_loss, noobject_loss, coord_loss], \n",
    "                                           predict, label, nilboy])\n",
    "\n",
    "            for j in range(4):\n",
    "                loss[j] = loss[j] + tuple_results[2][j]\n",
    "            nilboy = tuple_results[5]\n",
    "\n",
    "        tf.add_to_collection('losses', (loss[0] + loss[1] + loss[2] + loss[3])/self.batch_size)\n",
    "\n",
    "        tf.summary.scalar('class_loss', loss[0]/self.batch_size)\n",
    "        tf.summary.scalar('object_loss', loss[1]/self.batch_size)\n",
    "        tf.summary.scalar('noobject_loss', loss[2]/self.batch_size)\n",
    "        tf.summary.scalar('coord_loss', loss[3]/self.batch_size)\n",
    "        tf.summary.scalar('weight_loss', tf.add_n(tf.get_collection('losses')) \n",
    "                          - (loss[0] + loss[1] + loss[2] + loss[3])/self.batch_size )\n",
    "\n",
    "        return tf.add_n(tf.get_collection('losses'), name='total_loss'), nilboy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YoloRunner(object):\n",
    "\n",
    "    def __init__(self, dataset, net, common_params, solver_params):\n",
    "        #process params\n",
    "        self.learning_rate = float(solver_params['learning_rate'])\n",
    "        self.decay_steps = float(solver_params['decay_steps'])\n",
    "        self.decay_rate = float(solver_params['decay_rate'])\n",
    "        self.staircase = float(solver_params['staircase'])\n",
    "        self.moment = float(solver_params['moment'])\n",
    "        self.batch_size = int(common_params['batch_size'])\n",
    "        self.height = int(common_params['image_size'])\n",
    "        self.width = int(common_params['image_size'])\n",
    "        self.max_objects = int(common_params['max_objects_per_image'])\n",
    "        self.train_dir = str(solver_params['train_dir'])\n",
    "        \n",
    "        if not os.path.exists(self.train_dir):\n",
    "            os.makedirs(self.train_dir)\n",
    "        \n",
    "        self.max_iterators = int(solver_params['max_iterators'])\n",
    "        self.print_frequency = int(solver_params['print_frequency'])\n",
    "        self.save_frequency = int(solver_params['save_frequency'])\n",
    "        self.log_file = solver_params['log_file']\n",
    "        #\n",
    "        self.dataset = dataset\n",
    "        self.net = net\n",
    "        #construct graph\n",
    "        self.construct_graph()\n",
    "    def _train(self):\n",
    "\n",
    "        #opt = tf.train.MomentumOptimizer(self.learning_rate, self.moment)\n",
    "        _learning_rate = tf.train.exponential_decay(\n",
    "            self.learning_rate, self.global_step, self.decay_steps,\n",
    "            self.decay_rate, self.staircase, name='learning_rate')\n",
    "        \n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        grads = opt.compute_gradients(self.total_loss)\n",
    "        apply_gradient_op = opt.apply_gradients(grads, global_step=self.global_step)\n",
    "\n",
    "        return apply_gradient_op\n",
    "\n",
    "    def construct_graph(self):\n",
    "        # construct graph\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        self.images = tf.placeholder(tf.float32, (None, self.height, self.width, 3))\n",
    "        self.labels = tf.placeholder(tf.float32, (None, self.max_objects, 5))\n",
    "        self.objects_num = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "        self.predicts = self.net.inference(self.images)\n",
    "        self.total_loss, self.nilboy = self.net.loss(self.predicts, self.labels, self.objects_num)\n",
    "        tf.summary.scalar('loss', self.total_loss)\n",
    "        self.train_op = self._train()\n",
    "\n",
    "    def run(self):\n",
    "        saver = tf.train.Saver(self.net.trainable_collection, write_version=tf.train.SaverDef.V2)\n",
    "\n",
    "        init =  tf.global_variables_initializer()\n",
    "\n",
    "        summary_op = tf.summary.merge_all()\n",
    "\n",
    "        sess = tf.Session(config = config)\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        summary_writer = tf.summary.FileWriter(self.train_dir, sess.graph)\n",
    "\n",
    "        for step in range(self.max_iterators):\n",
    "            start_time = time.time()\n",
    "\n",
    "            np_images, np_labels, np_objects_num = self.dataset.batch()\n",
    "\n",
    "            _, loss_value, nilboy= sess.run([self.train_op, self.total_loss, self.nilboy], \n",
    "                                            feed_dict={self.images: np_images, \n",
    "                                                       self.labels: np_labels,\n",
    "                                                       self.objects_num: np_objects_num})\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "            if step % self.print_frequency== 0:\n",
    "                num_examples_per_step = self.dataset.batch_size\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = ('step %d, loss = %.5f (%.1f examples/sec; %.3f '\n",
    "                              'sec/batch)')\n",
    "                print (format_str % (step, loss_value,\n",
    "                                     examples_per_sec, sec_per_batch), file=open(self.log_file, \"a\"))\n",
    "                print (format_str % (step, loss_value,\n",
    "                                     examples_per_sec, sec_per_batch))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "            if step % self.print_frequency == 0:\n",
    "                summary_str = sess.run(summary_op, feed_dict={self.images: np_images, \n",
    "                                                              self.labels: np_labels, \n",
    "                                                              self.objects_num: np_objects_num})\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "            if step % self.save_frequency == 0:\n",
    "                saver.save(sess, self.train_dir + '/model.ckpt', global_step=step)\n",
    "        sess.close()\n",
    "\n",
    "    def prepare_inference(self, model_version):\n",
    "        saver = tf.train.Saver(self.net.trainable_collection)\n",
    "        init =  tf.global_variables_initializer()\n",
    "        \n",
    "        self.sess = tf.Session(config = config)\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        saver.restore(self.sess, self.train_dir+'/model.ckpt-'+model_version)\n",
    "\n",
    "    def make_one_prediction(self, images):\n",
    "\n",
    "        prediction = self.sess.run(self.predicts, feed_dict= {self.images: images})\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_params = {\n",
    "    'image_size': 448,\n",
    "    'batch_size': 2,\n",
    "    'num_classes': 20,\n",
    "    'max_objects_per_image': 20\n",
    "}\n",
    "dataset_params = {\n",
    "    'path': 'pascal_voc_training_data3.txt',\n",
    "    'image_dir': 'VOCdevkit_train/VOC2007/JPEGImages/',\n",
    "    'thread_num': 5\n",
    "}\n",
    "net_params = {\n",
    "    'weight_decay': 0.0005,\n",
    "    'cell_size': 7,\n",
    "    'boxes_per_cell': 2,\n",
    "    'object_scale': 1,\n",
    "    'noobject_scale': 0.5,\n",
    "    'class_scale': 1,\n",
    "    'coord_scale': 5, \n",
    "\n",
    "}\n",
    "solver_params = {\n",
    "    'learning_rate': 0.000001,\n",
    "    'decay_steps': 30000,\n",
    "    'decay_rate': 0.1,\n",
    "    'staircase': True,\n",
    "    'moment': 0.9,\n",
    "    'max_iterators': 20000,\n",
    "    'print_frequency': 100,\n",
    "    'save_frequency' : 1000,\n",
    "    'train_dir': 'models/yolo_v1_1',\n",
    "    'log_file' : 'yolo_v1_1.txt'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss = 18825.65234 (2.5 examples/sec; 0.806 sec/batch)\n",
      "step 100, loss = 1197.63489 (10.2 examples/sec; 0.197 sec/batch)\n",
      "step 200, loss = 490.72815 (9.9 examples/sec; 0.201 sec/batch)\n",
      "step 300, loss = 565.30890 (6.9 examples/sec; 0.291 sec/batch)\n",
      "step 400, loss = 379.61539 (6.8 examples/sec; 0.292 sec/batch)\n",
      "step 500, loss = 242.77216 (9.4 examples/sec; 0.212 sec/batch)\n",
      "step 600, loss = 225.91704 (6.3 examples/sec; 0.315 sec/batch)\n",
      "step 700, loss = 228.73366 (10.3 examples/sec; 0.195 sec/batch)\n",
      "step 800, loss = 159.45177 (10.2 examples/sec; 0.197 sec/batch)\n",
      "step 900, loss = 186.12831 (10.4 examples/sec; 0.193 sec/batch)\n",
      "step 1000, loss = 192.28249 (8.6 examples/sec; 0.232 sec/batch)\n",
      "step 1100, loss = 125.62882 (10.1 examples/sec; 0.198 sec/batch)\n",
      "step 1200, loss = 127.32304 (6.4 examples/sec; 0.314 sec/batch)\n",
      "step 1300, loss = 114.87353 (6.9 examples/sec; 0.290 sec/batch)\n",
      "step 1400, loss = 103.46947 (10.4 examples/sec; 0.193 sec/batch)\n",
      "step 1500, loss = 116.46519 (9.5 examples/sec; 0.211 sec/batch)\n",
      "step 1600, loss = 133.57088 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 1700, loss = 90.19458 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 1800, loss = 92.60155 (6.6 examples/sec; 0.302 sec/batch)\n",
      "step 1900, loss = 91.07961 (9.6 examples/sec; 0.209 sec/batch)\n",
      "step 2000, loss = 133.30180 (9.8 examples/sec; 0.205 sec/batch)\n",
      "step 2100, loss = 87.81300 (6.9 examples/sec; 0.292 sec/batch)\n",
      "step 2200, loss = 96.84291 (10.1 examples/sec; 0.198 sec/batch)\n",
      "step 2300, loss = 72.81199 (9.1 examples/sec; 0.221 sec/batch)\n",
      "step 2400, loss = 76.65475 (8.8 examples/sec; 0.226 sec/batch)\n",
      "step 2500, loss = 74.93217 (10.2 examples/sec; 0.195 sec/batch)\n",
      "step 2600, loss = 65.54916 (6.8 examples/sec; 0.293 sec/batch)\n",
      "step 2700, loss = 76.69424 (6.7 examples/sec; 0.299 sec/batch)\n",
      "step 2800, loss = 42.64705 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 2900, loss = 52.63256 (6.7 examples/sec; 0.298 sec/batch)\n",
      "step 3000, loss = 72.47273 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 3100, loss = 60.81254 (8.4 examples/sec; 0.238 sec/batch)\n",
      "step 3200, loss = 49.90334 (10.2 examples/sec; 0.196 sec/batch)\n",
      "step 3300, loss = 67.51534 (9.1 examples/sec; 0.220 sec/batch)\n",
      "step 3400, loss = 84.78159 (9.3 examples/sec; 0.214 sec/batch)\n",
      "step 3500, loss = 46.86203 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 3600, loss = 60.40281 (9.6 examples/sec; 0.208 sec/batch)\n",
      "step 3700, loss = 85.25809 (6.7 examples/sec; 0.296 sec/batch)\n",
      "step 3800, loss = 92.46529 (6.6 examples/sec; 0.305 sec/batch)\n",
      "step 3900, loss = 61.35699 (10.3 examples/sec; 0.195 sec/batch)\n",
      "step 4000, loss = 60.54019 (9.8 examples/sec; 0.203 sec/batch)\n",
      "step 4100, loss = 51.29612 (8.4 examples/sec; 0.239 sec/batch)\n",
      "step 4200, loss = 61.02560 (8.5 examples/sec; 0.235 sec/batch)\n",
      "step 4300, loss = 79.74335 (8.7 examples/sec; 0.230 sec/batch)\n",
      "step 4400, loss = 85.94701 (9.7 examples/sec; 0.206 sec/batch)\n",
      "step 4500, loss = 56.82487 (8.8 examples/sec; 0.227 sec/batch)\n",
      "step 4600, loss = 72.05202 (6.6 examples/sec; 0.305 sec/batch)\n",
      "step 4700, loss = 47.90341 (6.4 examples/sec; 0.315 sec/batch)\n",
      "step 4800, loss = 57.07195 (6.2 examples/sec; 0.323 sec/batch)\n",
      "step 4900, loss = 48.39310 (9.3 examples/sec; 0.216 sec/batch)\n",
      "step 5000, loss = 35.89104 (10.0 examples/sec; 0.199 sec/batch)\n",
      "step 5100, loss = 24.71785 (9.2 examples/sec; 0.218 sec/batch)\n",
      "step 5200, loss = 67.55616 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 5300, loss = 42.26954 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 5400, loss = 53.79281 (7.2 examples/sec; 0.278 sec/batch)\n",
      "step 5500, loss = 39.33837 (9.1 examples/sec; 0.220 sec/batch)\n",
      "step 5600, loss = 72.02965 (6.5 examples/sec; 0.305 sec/batch)\n",
      "step 5700, loss = 22.84869 (6.7 examples/sec; 0.300 sec/batch)\n",
      "step 5800, loss = 49.53011 (10.0 examples/sec; 0.199 sec/batch)\n",
      "step 5900, loss = 41.55647 (8.7 examples/sec; 0.229 sec/batch)\n",
      "step 6000, loss = 51.06596 (6.4 examples/sec; 0.315 sec/batch)\n",
      "step 6100, loss = 69.70398 (10.2 examples/sec; 0.197 sec/batch)\n",
      "step 6200, loss = 38.25651 (9.7 examples/sec; 0.206 sec/batch)\n",
      "step 6300, loss = 47.47903 (10.0 examples/sec; 0.200 sec/batch)\n",
      "step 6400, loss = 23.78803 (10.6 examples/sec; 0.189 sec/batch)\n",
      "step 6500, loss = 39.54319 (8.6 examples/sec; 0.234 sec/batch)\n",
      "step 6600, loss = 40.86030 (7.4 examples/sec; 0.269 sec/batch)\n",
      "step 6700, loss = 34.17281 (10.4 examples/sec; 0.192 sec/batch)\n",
      "step 6800, loss = 70.75252 (9.7 examples/sec; 0.206 sec/batch)\n",
      "step 6900, loss = 20.20123 (6.6 examples/sec; 0.304 sec/batch)\n",
      "step 7000, loss = 22.49799 (10.0 examples/sec; 0.199 sec/batch)\n",
      "step 7100, loss = 57.99627 (7.0 examples/sec; 0.286 sec/batch)\n",
      "step 7200, loss = 46.71726 (8.7 examples/sec; 0.229 sec/batch)\n",
      "step 7300, loss = 32.25470 (10.2 examples/sec; 0.196 sec/batch)\n",
      "step 7400, loss = 35.09390 (6.9 examples/sec; 0.288 sec/batch)\n",
      "step 7500, loss = 19.23059 (9.7 examples/sec; 0.207 sec/batch)\n",
      "step 7600, loss = 20.10142 (10.6 examples/sec; 0.189 sec/batch)\n",
      "step 7700, loss = 29.94166 (7.2 examples/sec; 0.278 sec/batch)\n",
      "step 7800, loss = 44.94747 (6.9 examples/sec; 0.288 sec/batch)\n",
      "step 7900, loss = 30.61124 (10.5 examples/sec; 0.191 sec/batch)\n",
      "step 8000, loss = 27.03569 (9.2 examples/sec; 0.217 sec/batch)\n",
      "step 8100, loss = 42.00082 (10.0 examples/sec; 0.200 sec/batch)\n",
      "step 8200, loss = 19.00966 (6.6 examples/sec; 0.301 sec/batch)\n",
      "step 8300, loss = 31.87035 (10.3 examples/sec; 0.194 sec/batch)\n",
      "step 8400, loss = 36.27591 (10.0 examples/sec; 0.201 sec/batch)\n",
      "step 8500, loss = 29.18171 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 8600, loss = 65.06203 (8.3 examples/sec; 0.240 sec/batch)\n",
      "step 8700, loss = 40.61877 (8.4 examples/sec; 0.239 sec/batch)\n",
      "step 8800, loss = 51.84170 (9.5 examples/sec; 0.211 sec/batch)\n",
      "step 8900, loss = 30.54422 (9.8 examples/sec; 0.205 sec/batch)\n",
      "step 9000, loss = 35.16072 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 9100, loss = 34.99826 (10.1 examples/sec; 0.199 sec/batch)\n",
      "step 9200, loss = 40.95258 (6.4 examples/sec; 0.314 sec/batch)\n",
      "step 9300, loss = 39.38108 (10.0 examples/sec; 0.200 sec/batch)\n",
      "step 9400, loss = 50.33684 (9.7 examples/sec; 0.206 sec/batch)\n",
      "step 9500, loss = 29.37967 (10.8 examples/sec; 0.186 sec/batch)\n",
      "step 9600, loss = 34.18273 (6.5 examples/sec; 0.305 sec/batch)\n",
      "step 9700, loss = 26.76474 (6.7 examples/sec; 0.300 sec/batch)\n",
      "step 9800, loss = 24.07020 (10.1 examples/sec; 0.197 sec/batch)\n",
      "step 9900, loss = 38.27264 (6.6 examples/sec; 0.304 sec/batch)\n",
      "step 10000, loss = 43.87907 (6.4 examples/sec; 0.312 sec/batch)\n",
      "step 10100, loss = 28.25940 (10.1 examples/sec; 0.197 sec/batch)\n",
      "step 10200, loss = 33.28770 (9.2 examples/sec; 0.218 sec/batch)\n",
      "step 10300, loss = 25.75612 (8.7 examples/sec; 0.231 sec/batch)\n",
      "step 10400, loss = 30.04659 (9.0 examples/sec; 0.222 sec/batch)\n",
      "step 10500, loss = 44.71147 (6.8 examples/sec; 0.296 sec/batch)\n",
      "step 10600, loss = 16.25954 (10.6 examples/sec; 0.189 sec/batch)\n",
      "step 10700, loss = 25.38870 (8.3 examples/sec; 0.242 sec/batch)\n",
      "step 10800, loss = 38.58803 (8.6 examples/sec; 0.231 sec/batch)\n",
      "step 10900, loss = 58.22983 (9.0 examples/sec; 0.221 sec/batch)\n",
      "step 11000, loss = 60.01689 (9.5 examples/sec; 0.209 sec/batch)\n",
      "step 11100, loss = 28.22012 (10.1 examples/sec; 0.199 sec/batch)\n",
      "step 11200, loss = 46.69857 (8.5 examples/sec; 0.236 sec/batch)\n",
      "step 11300, loss = 34.05077 (6.8 examples/sec; 0.295 sec/batch)\n",
      "step 11400, loss = 37.65326 (6.5 examples/sec; 0.307 sec/batch)\n",
      "step 11500, loss = 23.97633 (10.4 examples/sec; 0.193 sec/batch)\n",
      "step 11600, loss = 28.93444 (9.0 examples/sec; 0.222 sec/batch)\n",
      "step 11700, loss = 35.31224 (9.8 examples/sec; 0.204 sec/batch)\n",
      "step 11800, loss = 45.70794 (8.4 examples/sec; 0.239 sec/batch)\n",
      "step 11900, loss = 30.41314 (6.6 examples/sec; 0.302 sec/batch)\n",
      "step 12000, loss = 36.77815 (6.3 examples/sec; 0.317 sec/batch)\n",
      "step 12100, loss = 54.68875 (10.2 examples/sec; 0.196 sec/batch)\n",
      "step 12200, loss = 58.17497 (8.8 examples/sec; 0.226 sec/batch)\n",
      "step 12300, loss = 41.70454 (10.1 examples/sec; 0.198 sec/batch)\n",
      "step 12400, loss = 44.77777 (6.5 examples/sec; 0.307 sec/batch)\n",
      "step 12500, loss = 29.85500 (8.9 examples/sec; 0.224 sec/batch)\n",
      "step 12600, loss = 30.45142 (10.1 examples/sec; 0.198 sec/batch)\n",
      "step 12700, loss = 20.26152 (9.3 examples/sec; 0.215 sec/batch)\n",
      "step 12800, loss = 34.71713 (9.0 examples/sec; 0.223 sec/batch)\n",
      "step 12900, loss = 40.86449 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 13000, loss = 43.94955 (9.1 examples/sec; 0.221 sec/batch)\n",
      "step 13100, loss = 23.95621 (7.6 examples/sec; 0.262 sec/batch)\n",
      "step 13200, loss = 29.88003 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 13300, loss = 21.73623 (10.3 examples/sec; 0.194 sec/batch)\n",
      "step 13400, loss = 19.63967 (9.9 examples/sec; 0.202 sec/batch)\n",
      "step 13500, loss = 13.95544 (6.4 examples/sec; 0.312 sec/batch)\n",
      "step 13600, loss = 43.17937 (10.0 examples/sec; 0.201 sec/batch)\n",
      "step 13700, loss = 23.31927 (6.8 examples/sec; 0.295 sec/batch)\n",
      "step 13800, loss = 29.34731 (6.5 examples/sec; 0.309 sec/batch)\n",
      "step 13900, loss = 24.79021 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 14000, loss = 56.00978 (6.6 examples/sec; 0.304 sec/batch)\n",
      "step 14100, loss = 39.37706 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 14200, loss = 13.90837 (10.9 examples/sec; 0.183 sec/batch)\n",
      "step 14300, loss = 22.73369 (10.5 examples/sec; 0.191 sec/batch)\n",
      "step 14400, loss = 33.88629 (8.5 examples/sec; 0.234 sec/batch)\n",
      "step 14500, loss = 20.56024 (9.0 examples/sec; 0.222 sec/batch)\n",
      "step 14600, loss = 26.50277 (6.8 examples/sec; 0.293 sec/batch)\n",
      "step 14700, loss = 13.37587 (5.7 examples/sec; 0.348 sec/batch)\n",
      "step 14800, loss = 41.79932 (8.6 examples/sec; 0.232 sec/batch)\n",
      "step 14900, loss = 28.04716 (9.5 examples/sec; 0.212 sec/batch)\n",
      "step 15000, loss = 33.28595 (9.6 examples/sec; 0.208 sec/batch)\n",
      "step 15100, loss = 51.71981 (8.5 examples/sec; 0.235 sec/batch)\n",
      "step 15200, loss = 26.14011 (6.7 examples/sec; 0.299 sec/batch)\n",
      "step 15300, loss = 13.13334 (8.6 examples/sec; 0.232 sec/batch)\n",
      "step 15400, loss = 13.35892 (10.2 examples/sec; 0.196 sec/batch)\n",
      "step 15500, loss = 27.67285 (8.7 examples/sec; 0.230 sec/batch)\n",
      "step 15600, loss = 27.94746 (8.1 examples/sec; 0.248 sec/batch)\n",
      "step 15700, loss = 51.26284 (6.4 examples/sec; 0.311 sec/batch)\n",
      "step 15800, loss = 19.19149 (6.7 examples/sec; 0.298 sec/batch)\n",
      "step 15900, loss = 29.18815 (9.9 examples/sec; 0.202 sec/batch)\n",
      "step 16000, loss = 40.61735 (8.0 examples/sec; 0.249 sec/batch)\n",
      "step 16100, loss = 21.20261 (9.4 examples/sec; 0.213 sec/batch)\n",
      "step 16200, loss = 27.53745 (6.3 examples/sec; 0.318 sec/batch)\n",
      "step 16300, loss = 50.82666 (6.6 examples/sec; 0.305 sec/batch)\n",
      "step 16400, loss = 53.78872 (10.0 examples/sec; 0.200 sec/batch)\n",
      "step 16500, loss = 35.15612 (8.9 examples/sec; 0.224 sec/batch)\n",
      "step 16600, loss = 40.06985 (9.7 examples/sec; 0.207 sec/batch)\n",
      "step 16700, loss = 20.75806 (10.4 examples/sec; 0.192 sec/batch)\n",
      "step 16800, loss = 31.67495 (8.3 examples/sec; 0.240 sec/batch)\n",
      "step 16900, loss = 18.58664 (10.5 examples/sec; 0.190 sec/batch)\n",
      "step 17000, loss = 53.31571 (9.7 examples/sec; 0.206 sec/batch)\n",
      "step 17100, loss = 28.03628 (9.1 examples/sec; 0.221 sec/batch)\n",
      "step 17200, loss = 39.56504 (6.3 examples/sec; 0.317 sec/batch)\n",
      "step 17300, loss = 20.33455 (6.6 examples/sec; 0.302 sec/batch)\n",
      "step 17400, loss = 26.79929 (8.5 examples/sec; 0.236 sec/batch)\n",
      "step 17500, loss = 18.16661 (6.7 examples/sec; 0.297 sec/batch)\n",
      "step 17600, loss = 52.86133 (6.6 examples/sec; 0.303 sec/batch)\n",
      "step 17700, loss = 12.33518 (6.5 examples/sec; 0.309 sec/batch)\n",
      "step 17800, loss = 27.51390 (7.9 examples/sec; 0.253 sec/batch)\n",
      "step 17900, loss = 19.93283 (10.6 examples/sec; 0.188 sec/batch)\n",
      "step 18000, loss = 30.74518 (9.5 examples/sec; 0.210 sec/batch)\n",
      "step 18100, loss = 30.67130 (6.8 examples/sec; 0.294 sec/batch)\n",
      "step 18200, loss = 52.43255 (6.3 examples/sec; 0.316 sec/batch)\n",
      "step 18300, loss = 27.15529 (9.5 examples/sec; 0.211 sec/batch)\n",
      "step 18400, loss = 38.63826 (9.4 examples/sec; 0.212 sec/batch)\n",
      "step 18500, loss = 19.75802 (6.6 examples/sec; 0.304 sec/batch)\n",
      "step 18600, loss = 30.31497 (9.0 examples/sec; 0.223 sec/batch)\n",
      "step 18700, loss = 21.14893 (6.7 examples/sec; 0.297 sec/batch)\n",
      "step 18800, loss = 52.01784 (6.6 examples/sec; 0.305 sec/batch)\n",
      "step 18900, loss = 26.41863 (9.7 examples/sec; 0.207 sec/batch)\n",
      "step 19000, loss = 38.21154 (6.9 examples/sec; 0.289 sec/batch)\n",
      "step 19100, loss = 26.87389 (8.8 examples/sec; 0.229 sec/batch)\n",
      "step 19200, loss = 25.89525 (6.1 examples/sec; 0.329 sec/batch)\n",
      "step 19300, loss = 48.71799 (10.2 examples/sec; 0.197 sec/batch)\n",
      "step 19400, loss = 51.64954 (7.3 examples/sec; 0.275 sec/batch)\n",
      "step 19500, loss = 11.84769 (9.1 examples/sec; 0.220 sec/batch)\n",
      "step 19600, loss = 11.91932 (8.4 examples/sec; 0.237 sec/batch)\n",
      "step 19700, loss = 18.81021 (7.0 examples/sec; 0.284 sec/batch)\n",
      "step 19800, loss = 29.53383 (6.3 examples/sec; 0.320 sec/batch)\n",
      "step 19900, loss = 48.16801 (6.9 examples/sec; 0.292 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sys.path.append('./')\n",
    "\n",
    "dataset = DatasetRunner(common_params, dataset_params)\n",
    "net = YoloTinyNet(common_params, net_params)\n",
    "model_runner = YoloRunner(dataset, net, common_params, solver_params)\n",
    "\n",
    "model_runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_predicts(predicts):\n",
    "    \"\"\"\n",
    "    Process YOLO outputs into bou\n",
    "    \"\"\"\n",
    "    p_classes = predicts[0, :, :, 0:20]\n",
    "    C = predicts[0, :, :, 20:22]\n",
    "    coordinate = predicts[0, :, :, 22:]\n",
    "\n",
    "    p_classes = np.reshape(p_classes, (net_params['cell_size'], net_params['cell_size'], 1, 20))\n",
    "    C = np.reshape(C, (net_params['cell_size'], net_params['cell_size'], net_params['boxes_per_cell'], 1))\n",
    "    \n",
    "    P = C * p_classes\n",
    "    #P's shape [7, 7, 2, 20]\n",
    "    #print P[5,1, 0, :]\n",
    "\n",
    "    #choose the most confidence one\n",
    "    max_conf = np.max(P)\n",
    "    index = np.argmax(P)\n",
    "\n",
    "    index = np.unravel_index(index, P.shape)\n",
    "\n",
    "    class_num = index[3]\n",
    "\n",
    "    coordinate = np.reshape(coordinate, \n",
    "                            (net_params['cell_size'], \n",
    "                             net_params['cell_size'], \n",
    "                             net_params['boxes_per_cell'], \n",
    "                             4))\n",
    "\n",
    "    max_coordinate = coordinate[index[0], index[1], index[2], :]\n",
    "\n",
    "    xcenter = max_coordinate[0]\n",
    "    ycenter = max_coordinate[1]\n",
    "    w = max_coordinate[2]\n",
    "    h = max_coordinate[3]\n",
    "\n",
    "    xcenter = (index[1] + xcenter) * (common_params['image_size']/float(net_params['cell_size']))\n",
    "    ycenter = (index[0] + ycenter) * (common_params['image_size']/float(net_params['cell_size']))\n",
    "\n",
    "    w = w * common_params['image_size']\n",
    "    h = h * common_params['image_size']\n",
    "\n",
    "    xmin = xcenter - w/2.0\n",
    "    ymin = ycenter - h/2.0\n",
    "\n",
    "    xmax = xmin + w\n",
    "    ymax = ymin + h\n",
    "\n",
    "    return xmin, ymin, xmax, ymax, class_num, max_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "common_params['batch_size'] = 1\n",
    "net = YoloTinyNet(common_params, net_params)\n",
    "solver = YoloRunner(None, net, common_params, solver_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_img_files = open('./pascal_voc_testing_data.txt')\n",
    "test_img_dir = './VOCdevkit_test/VOC2007/JPEGImages/'\n",
    "test_images = []\n",
    "img_size = common_params['image_size']\n",
    "\n",
    "for line in test_img_files:\n",
    "    line = line.strip()\n",
    "    ss = line.split(' ')\n",
    "    test_images.append(ss[0])\n",
    "    \n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images)\n",
    "\n",
    "def load_img_data(image_name):\n",
    "    \n",
    "    image_file = tf.read_file(test_img_dir+image_name)\n",
    "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "    \n",
    "    h = tf.shape(image)[0]\n",
    "    w = tf.shape(image)[1]\n",
    "\n",
    "    image = tf.image.resize_images(image, size=[img_size,img_size])\n",
    "    \n",
    "    return image_name, image, h, w\n",
    "test_dataset = test_dataset.map(load_img_data)\n",
    "test_iterator = test_dataset.make_one_shot_iterator()\n",
    "next_test_element = test_iterator.get_next()\n",
    "tdata_sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/yolo_v1_1/model.ckpt-19000\n",
      "done predicting all test data\n"
     ]
    }
   ],
   "source": [
    "output_file = open('./predict/my_test_predion_v1_1.txt', 'w')\n",
    "solver.prepare_inference(model_version = '19000')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        img_name, test_img, img_h, img_w = tdata_sess.run(next_test_element)\n",
    "        test_img = test_img/255 * 2 - 1\n",
    "        test_img = np.expand_dims(test_img, axis=0)\n",
    "        y_pred = solver.make_one_prediction(test_img)\n",
    "        xmin, ymin, xmax, ymax, class_num, conf = process_predicts(y_pred)\n",
    "\n",
    "        xmin, ymin, xmax, ymax = xmin*(img_w/img_size), ymin*(img_h/img_size), xmax*(img_w/img_size), ymax*(img_h/img_size)\n",
    "\n",
    "        #img filename, (xmin, ymin, xmax, ymax, class, confidence)*number_of_predictions\n",
    "        output_file.write(img_name.decode('ascii')+\" %d %d %d %d %d %f\\n\" %(xmin, ymin, xmax, ymax, class_num, conf))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"done predicting all test data\")\n",
    "        break\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Evalutation\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, './evaluate')\n",
    "import evaluate\n",
    "#evaluate.evaluate(\"input prediction file name\", \"desire output csv file name\")\n",
    "evaluate.evaluate('./predict/my_test_predion_v1_1.txt', './predict/my_output_file_v1_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_img = cv2.imread('./img_src/demo.jpg')\n",
    "resized_img = cv2.resize(np_img, (common_params['image_size'], common_params['image_size']))\n",
    "np_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB)\n",
    "resized_img = np_img\n",
    "np_img = np_img.astype(np.float32)\n",
    "np_img = np_img / 255.0 * 2 - 1\n",
    "np_img = np.reshape(np_img, (1, common_params['image_size'], common_params['image_size'], 3))\n",
    "\n",
    "y_pred = solver.make_one_prediction(np_img)\n",
    "xmin, ymin, xmax, ymax, class_num, conf = process_predicts(y_pred)\n",
    "class_name = classes_name[class_num]\n",
    "cv2.rectangle(resized_img, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 255), 3)\n",
    "cv2.putText(resized_img, class_name, (0, 200), 2, 1.5, (0, 255, 255), 2)\n",
    "\n",
    "plt.imshow(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.transpose(np.reshape(np.array([np.arange(7)] *7 * 2),(2, 7, 7)), (1, 2, 0))\n",
    "#np.reshape(np.array([np.arange(7)] *7 * 2),(7, 7, 2))\n",
    "\n",
    "#np.array([np.arange(7)] *7 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ros]",
   "language": "python",
   "name": "conda-env-ros-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
